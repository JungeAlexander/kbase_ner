{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hungry-scout",
   "metadata": {},
   "source": [
    "# Load SageMaker GroundTruth annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-brighton",
   "metadata": {},
   "source": [
    "## Load task manifest file mapping text sample IDs to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import codecs\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_path = Path(\"annotations/glue-dir-kbase-dev-sagemaker-ground-truth-labeling-clone/annotations/intermediate/1/annotations.manifest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert manifest_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_text_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(manifest_path, \"rt\") as fin:\n",
    "    for line in fin:\n",
    "        columns = line.split(\"\\t\")\n",
    "        index = int(columns[0])\n",
    "        text = \"\\t\".join(columns[1:-1])\n",
    "        _ = columns[-1]  # no idea what this column is\n",
    "        assert index not in index_to_text_raw\n",
    "        index_to_text_raw[index] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-disney",
   "metadata": {},
   "source": [
    "## Load annotation for given sample text IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = Path(\"annotations/glue-dir-kbase-dev-sagemaker-ground-truth-labeling-clone/annotations/worker-response/iteration-1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert annotations_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_annotation_raw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotations_subdir in annotations_dir.iterdir():\n",
    "    index = int(annotations_subdir.name)\n",
    "    for i, annotations_file in enumerate(annotations_subdir.iterdir()):\n",
    "        assert i == 0, f\"found more than one annotation in {annotations_subdir}\"\n",
    "        with open(annotations_file, \"rt\") as fin:\n",
    "            j = json.load(fin)\n",
    "        assert index not in index_to_annotation_raw\n",
    "        answers = j[\"answers\"]\n",
    "        assert len(answers) == 1\n",
    "        entities = answers[0]['answerContent']['crowd-entity-annotation']['entities']\n",
    "        index_to_annotation_raw[index] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-scientist",
   "metadata": {},
   "source": [
    "## Merge text and annotation whole removing episode IDs from text and adjusting entity positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EntityMatch:\n",
    "    label: str\n",
    "    start_offset: int\n",
    "    end_offset: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Doc:\n",
    "    id_: str\n",
    "    text: str\n",
    "    annotations: List[EntityMatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_to_doc = {}\n",
    "for index, annotation_raw in index_to_annotation_raw.items():\n",
    "    text_raw = index_to_text_raw[index]\n",
    "    text = codecs.unicode_escape_decode(text_raw)[0]\n",
    "    id_, text = text.split(\"\\n\", 1)\n",
    "    id_offset = len(id_) + 1  # +1 due to newline which was stripped of before\n",
    "    entity_matches = []\n",
    "    for a in annotation_raw:\n",
    "        match = EntityMatch(label=a[\"label\"],\n",
    "                            start_offset=a[\"startOffset\"] - id_offset,\n",
    "                            end_offset=a[\"endOffset\"] - id_offset)\n",
    "        entity_matches.append(match)\n",
    "    doc = Doc(id_=id_, text=text, annotations=entity_matches)\n",
    "    assert id_ not in doc_id_to_doc\n",
    "    doc_id_to_doc[id_] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-badge",
   "metadata": {},
   "source": [
    "## Create spacy Doc objects, load entity annotations and write DocBin to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy English NER labels https://spacy.io/models/en#en_core_web_sm-labels\n",
    "# spacy glossary: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "label_to_spacy_ner_label = {\n",
    "    \"Book\": \"WORK_OF_ART\",\n",
    "    \"Person\": \"PERSON\",\n",
    "    \"Software\": \"PRODUCT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_spacy_docs = {}\n",
    "check_ents = defaultdict(list)\n",
    "for d in doc_id_to_doc.values():\n",
    "    spacy_doc = nlp(d.text.encode('utf8','replace').decode('utf8')) # TODO is this necessary?\n",
    "    my_ents = []\n",
    "    for a in d.annotations:\n",
    "        ent = spacy_doc.char_span(a.start_offset,\n",
    "                                  a.end_offset,\n",
    "                                  label=label_to_spacy_ner_label[a.label],\n",
    "                                  alignment_mode=\"expand\"\n",
    "                                 )\n",
    "        assert ent is not None\n",
    "        my_ents.append(ent)\n",
    "    assert len(my_ents) > 0\n",
    "    \n",
    "    # keep only the first entity if they overlap\n",
    "    tokens_covered = set()\n",
    "    non_overlapping_ents = []\n",
    "    for ent in my_ents:\n",
    "        keep_ent = True\n",
    "        ent_tokens = set()\n",
    "        for tok in range(ent.start, ent.end):\n",
    "            if tok in tokens_covered:\n",
    "                check_ents[d.id_].append(ent)\n",
    "                keep_ent = False\n",
    "                continue\n",
    "            ent_tokens.add(tok)\n",
    "        if keep_ent:\n",
    "            tokens_covered.update(ent_tokens)\n",
    "            non_overlapping_ents.append(ent)\n",
    "    assert len(non_overlapping_ents) > 0\n",
    "        \n",
    "    spacy_doc.user_data = {\"id\": d.id_}\n",
    "    # TODO keep spacy_doc.ents from default pipeline by setting default=\"unmodified\" below?\n",
    "    spacy_doc.set_ents(non_overlapping_ents, default=\"missing\")\n",
    "    assert d.id_ not in id_to_spacy_docs\n",
    "    id_to_spacy_docs[d.id_] = spacy_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect entities\n",
    "for id_,ents in check_ents.items():\n",
    "    for e in ents:\n",
    "        print(f\"{id_} -- {e.label} -- '{e.as_doc()}'\")\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-electric",
   "metadata": {},
   "source": [
    "## Visualize for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(id_to_spacy_docs[\"PythonBytes:91\"], style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(id_to_spacy_docs[\"PythonBytes:100\"], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-customer",
   "metadata": {},
   "source": [
    "## Split into train/dev/test per podcast, taking time into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_episode = [x.split(\":\") for x in id_to_spacy_docs.keys()]\n",
    "podcast_to_sorted_episode = defaultdict(list)\n",
    "for podcast, episode in podcast_episode:\n",
    "    episode = int(episode)\n",
    "    podcast_to_sorted_episode[podcast].append(episode)\n",
    "for episodes in podcast_to_sorted_episode.values():\n",
    "    episodes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "dev_ids = []\n",
    "test_ids = []\n",
    "for p, es in podcast_to_sorted_episode.items():\n",
    "    train, dev_test = train_test_split(es, test_size=0.3, shuffle=False)\n",
    "    dev, test = train_test_split(dev_test, test_size=0.5, shuffle=False)\n",
    "    train_ids.extend((p + \":\" + str(t) for t in train))\n",
    "    dev_ids.extend((p + \":\" + str(d) for d in dev))\n",
    "    test_ids.extend((p + \":\" + str(t) for t in test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ids in zip((\"train\", \"dev\", \"test\"), (train_ids, dev_ids, test_ids)):\n",
    "    output_path = Path(f\"./{name}.spacy\")\n",
    "    assert not output_path.exists()\n",
    "    current_docs = [id_to_spacy_docs[id_] for id_ in ids]\n",
    "    doc_bin = spacy.tokens.DocBin(docs=current_docs)\n",
    "    doc_bin.to_disk(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
